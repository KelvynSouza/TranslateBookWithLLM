version: '3.8'

services:
  translatebook:
    build: .
    image: translatebook:latest
    container_name: translatebook_app
    ports:
      - "${PORT:-5000}:${PORT:-5000}"
    volumes:
      - ./translated_files:/app/translated_files
      - ./.env:/app/.env:ro  # Optional: mount .env file if using local config
    environment:
      - PORT=${PORT:-5000}
      - API_ENDPOINT=${API_ENDPOINT:-http://localhost:11434/api/generate}
      - DEFAULT_MODEL=${DEFAULT_MODEL:-mistral-small:24b}
    restart: unless-stopped
    networks:
      - translatebook-network

  # Optional: Ollama service for local LLM
  # Uncomment if you want to run Ollama in Docker
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: ollama_service
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   restart: unless-stopped
  #   networks:
  #     - translatebook-network

networks:
  translatebook-network:
    driver: bridge

# volumes:
#   ollama_data: