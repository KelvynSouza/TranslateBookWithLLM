"""
Translation module for LLM communication
"""
import asyncio
import time
import re
from tqdm.auto import tqdm

from src.config import (
    DEFAULT_MODEL, TRANSLATE_TAG_IN, TRANSLATE_TAG_OUT
)
from prompts import generate_translation_prompt, generate_subtitle_block_prompt, generate_post_processing_prompt
from .llm_client import default_client, LLMClient
from typing import List, Dict, Tuple, Optional


def _clean_html_entities(text):
    """
    Clean up common HTML entity issues in translated text.
    
    Args:
        text (str): Text that may contain HTML entities
        
    Returns:
        str: Cleaned text
    """
    if not text:
        return text
    
    # Common patterns where LLMs generate HTML entities incorrectly
    # Replace multiple &nbsp; entities with actual non-breaking spaces
    text = re.sub(r'(&nbsp;)+', lambda m: '\u00A0' * (len(m.group()) // 6), text)
    
    # Replace other common HTML entities
    text = text.replace('&amp;', '&')
    text = text.replace('&lt;', '<')
    text = text.replace('&gt;', '>')
    text = text.replace('&quot;', '"')
    text = text.replace('&#39;', "'")
    text = text.replace('&apos;', "'")
    
    return text


def _create_llm_client(llm_provider: str, gemini_api_key: Optional[str], 
                      api_endpoint: str, model_name: str) -> Optional[LLMClient]:
    """
    Create LLM client based on provider or custom endpoint
    
    Args:
        llm_provider: Provider type ('ollama' or 'gemini')
        gemini_api_key: API key for Gemini provider
        api_endpoint: API endpoint for custom Ollama instance
        model_name: Model name to use
        
    Returns:
        LLMClient instance or None if using default client
    """
    if llm_provider == "gemini" and gemini_api_key:
        return LLMClient(provider_type="gemini", api_key=gemini_api_key, model=model_name)
    elif api_endpoint and api_endpoint != default_client.api_endpoint:
        return LLMClient(provider_type="ollama", api_endpoint=api_endpoint, model=model_name)
    return None


async def generate_translation_request(main_content, context_before, context_after, previous_translation_context,
                                       source_language="English", target_language="French", model=DEFAULT_MODEL,
                                       llm_client=None, log_callback=None, custom_instructions=""):
    """
    Generate translation request to LLM API
    
    Args:
        main_content (str): Text to translate
        context_before (str): Context before main content
        context_after (str): Context after main content
        previous_translation_context (str): Previous translation for consistency
        source_language (str): Source language
        target_language (str): Target language
        model (str): LLM model name
        api_endpoint_param (str): API endpoint to use
        log_callback (callable): Logging callback function
        custom_instructions (str): Additional translation instructions
        
    Returns:
        str: Translated text or None if failed
    """
    # Skip LLM translation for single character or empty chunks
    if len(main_content.strip()) <= 1:
        if log_callback:
            log_callback("skip_translation", f"Skipping LLM for single/empty character: '{main_content}'")
        return main_content
    
    structured_prompt = generate_translation_prompt(
        main_content, 
        context_before, 
        context_after, 
        previous_translation_context,
        source_language, 
        target_language,
        custom_instructions=custom_instructions
    )
    
    print("\n-------SENT to LLM-------")
    print(structured_prompt)
    print("-------SENT to LLM-------\n")

    start_time = time.time()
    
    # Use provided client or default
    client = llm_client or default_client
    full_raw_response = await client.make_request(structured_prompt, model)
    execution_time = time.time() - start_time

    if not full_raw_response:
        err_msg = "ERROR: LLM API request failed"
        if log_callback: 
            log_callback("llm_api_error", err_msg)
        else: 
            tqdm.write(f"\n{err_msg}")
        return None

    print("\n-------LLM RESPONSE-------")
    print(full_raw_response)
    print("-------LLM RESPONSE-------\n")

    translated_text = client.extract_translation(full_raw_response)
    
    if translated_text:
        # Clean up any HTML entities that might have been generated by the LLM
        return _clean_html_entities(translated_text)
    else:
        warn_msg = f"WARNING: Translation tags missing in LLM response."
        if log_callback:
            log_callback("llm_tag_warning", warn_msg)
            log_callback("llm_raw_response_preview", f"LLM raw response: {full_raw_response[:500]}...")
        else:
            tqdm.write(f"\n{warn_msg} Excerpt: {full_raw_response[:100]}...")

        if main_content in full_raw_response:
            discard_msg = "WARNING: LLM response seems to contain input. Discarded."
            if log_callback: 
                log_callback("llm_prompt_in_response_warning", discard_msg)
            else: 
                tqdm.write(discard_msg)
            return None
        # Clean up any HTML entities even in the fallback case
        return _clean_html_entities(full_raw_response.strip())


async def post_process_translation(translated_text, target_language="French", model=DEFAULT_MODEL,
                                 llm_client=None, log_callback=None, custom_instructions="",
                                 context_before="", context_after=""):
    """
    Post-process translated text to improve quality
    
    Args:
        translated_text (str): Previously translated text to improve
        target_language (str): Target language
        model (str): LLM model name
        llm_client: LLM client instance
        log_callback (callable): Logging callback function
        custom_instructions (str): Additional improvement instructions
        context_before (str): Translated context before this text
        context_after (str): Translated context after this text
        
    Returns:
        str: Improved text or original if post-processing fails
    """
    # Skip post-processing for very short text
    if len(translated_text.strip()) <= 1:
        return translated_text
    
    structured_prompt = generate_post_processing_prompt(
        translated_text,
        target_language,
        custom_instructions=custom_instructions,
        context_before=context_before,
        context_after=context_after
    )
    
    print("\n-------POST-PROCESSING SENT to LLM-------")
    print(structured_prompt)
    print("-------POST-PROCESSING SENT to LLM-------\n")
    
    start_time = time.time()
    
    # Use provided client or default
    client = llm_client or default_client
    full_raw_response = await client.make_request(structured_prompt, model)
    execution_time = time.time() - start_time
    
    if not full_raw_response:
        err_msg = "ERROR: Post-processing LLM API request failed"
        if log_callback:
            log_callback("post_process_api_error", err_msg)
        else:
            tqdm.write(f"\n{err_msg}")
        return translated_text  # Return original if post-processing fails
    
    print("\n-------POST-PROCESSING LLM RESPONSE-------")
    print(full_raw_response)
    print("-------POST-PROCESSING LLM RESPONSE-------\n")
    
    improved_text = client.extract_translation(full_raw_response)
    
    if improved_text:
        return _clean_html_entities(improved_text)
    else:
        warn_msg = "WARNING: Post-processing tags missing in LLM response. Using original."
        if log_callback:
            log_callback("post_process_tag_warning", warn_msg)
        else:
            tqdm.write(f"\n{warn_msg}")
        return translated_text  # Return original if extraction fails


async def translate_chunks(chunks, source_language, target_language, model_name, 
                          api_endpoint, progress_callback=None, log_callback=None, 
                          stats_callback=None, check_interruption_callback=None, custom_instructions="",
                          llm_provider="ollama", gemini_api_key=None, enable_post_processing=False,
                          post_processing_instructions=""):
    """
    Translate a list of text chunks
    
    Args:
        chunks (list): List of chunk dictionaries
        source_language (str): Source language
        target_language (str): Target language
        model_name (str): LLM model name
        api_endpoint (str): API endpoint
        progress_callback (callable): Progress update callback
        log_callback (callable): Logging callback
        stats_callback (callable): Statistics update callback
        check_interruption_callback (callable): Interruption check callback
        
    Returns:
        list: List of translated chunks
    """
    total_chunks = len(chunks)
    full_translation_parts = []
    last_successful_llm_context = ""
    completed_chunks_count = 0
    failed_chunks_count = 0

    if log_callback: 
        log_callback("txt_translation_loop_start", "Starting segment translation...")

    # Create LLM client based on provider or custom endpoint
    llm_client = _create_llm_client(llm_provider, gemini_api_key, api_endpoint, model_name)

    iterator = tqdm(chunks, desc=f"Translating {source_language} to {target_language}", unit="seg") if not log_callback else chunks

    for i, chunk_data in enumerate(iterator):
        if check_interruption_callback and check_interruption_callback():
            if log_callback: 
                log_callback("txt_translation_interrupted", f"Translation process for segment {i+1}/{total_chunks} interrupted by user signal.")
            else: 
                tqdm.write(f"\nTranslation interrupted by user at segment {i+1}/{total_chunks}.")
            break

        if progress_callback and total_chunks > 0:
            progress_callback((i / total_chunks) * 100)
        
        # Log progress summary periodically
        if log_callback and i > 0 and i % 5 == 0:
            log_callback("", "info", {
                'type': 'progress'
            })

        main_content_to_translate = chunk_data["main_content"]
        context_before_text = chunk_data["context_before"]
        context_after_text = chunk_data["context_after"]

        if not main_content_to_translate.strip():
            full_translation_parts.append(main_content_to_translate)
            completed_chunks_count += 1
            if stats_callback and total_chunks > 0:
                stats_callback({'completed_chunks': completed_chunks_count, 'failed_chunks': failed_chunks_count})
            continue

        translated_chunk_text = await generate_translation_request(
            main_content_to_translate, context_before_text, context_after_text,
            last_successful_llm_context, source_language, target_language,
            model_name, llm_client=llm_client, log_callback=log_callback,
            custom_instructions=custom_instructions
        )

        if translated_chunk_text is not None:
            # Apply post-processing if enabled
            if enable_post_processing:
                if log_callback:
                    log_callback("post_processing_chunk", f"Post-processing chunk {i+1}/{total_chunks}")
                
                # Get context for post-processing
                pp_context_before = ""
                pp_context_after = ""
                
                # Get translated context before (from previous chunks)
                if i > 0 and full_translation_parts:
                    # Get last 150 words from previous translations
                    prev_text = " ".join(full_translation_parts[-min(3, len(full_translation_parts)):])
                    words = prev_text.split()
                    if len(words) > 150:
                        pp_context_before = " ".join(words[-150:])
                    else:
                        pp_context_before = prev_text
                
                # Get translated context after (look ahead in remaining chunks)
                if i + 1 < len(chunks):
                    next_chunk = chunks[i + 1]
                    pp_context_after = next_chunk.get("main_content", "")[:500]  # First 500 chars of next chunk
                
                improved_text = await post_process_translation(
                    translated_chunk_text,
                    target_language,
                    model_name,
                    llm_client=llm_client,
                    log_callback=log_callback,
                    custom_instructions=post_processing_instructions,
                    context_before=pp_context_before,
                    context_after=pp_context_after
                )
                translated_chunk_text = improved_text
            
            full_translation_parts.append(translated_chunk_text)
            completed_chunks_count += 1
            words = translated_chunk_text.split()
            if len(words) > 150:
                last_successful_llm_context = " ".join(words[-150:])
            else:
                last_successful_llm_context = translated_chunk_text
        else:
            err_msg_chunk = f"ERROR translating segment {i+1}. Original content preserved."
            if log_callback: 
                log_callback("txt_chunk_translation_error", err_msg_chunk)
            else: 
                tqdm.write(f"\n{err_msg_chunk}")
            error_placeholder = f"[TRANSLATION_ERROR SEGMENT {i+1}]\n{main_content_to_translate}\n[/TRANSLATION_ERROR SEGMENT {i+1}]"
            full_translation_parts.append(error_placeholder)
            failed_chunks_count += 1
            last_successful_llm_context = ""

        if stats_callback and total_chunks > 0:
            stats_callback({'completed_chunks': completed_chunks_count, 'failed_chunks': failed_chunks_count})

    return full_translation_parts


async def translate_subtitles(subtitles: List[Dict[str, str]], source_language: str, 
                            target_language: str, model_name: str, api_endpoint: str,
                            progress_callback=None, log_callback=None, 
                            stats_callback=None, check_interruption_callback=None, custom_instructions="",
                            llm_provider="ollama", gemini_api_key=None, enable_post_processing=False,
                            post_processing_instructions="") -> Dict[int, str]:
    """
    Translate subtitle entries preserving structure
    
    Args:
        subtitles (list): List of subtitle dictionaries from SRT parser
        source_language (str): Source language
        target_language (str): Target language
        model_name (str): LLM model name
        api_endpoint (str): API endpoint
        progress_callback (callable): Progress update callback
        log_callback (callable): Logging callback
        stats_callback (callable): Statistics update callback
        check_interruption_callback (callable): Interruption check callback
        
    Returns:
        dict: Mapping of subtitle index to translated text
    """
    total_subtitles = len(subtitles)
    translations = {}
    completed_count = 0
    failed_count = 0
    
    if log_callback:
        log_callback("srt_translation_start", f"Starting translation of {total_subtitles} subtitles...")
    
    # Create LLM client based on provider or custom endpoint
    llm_client = _create_llm_client(llm_provider, gemini_api_key, api_endpoint, model_name)
    
    iterator = tqdm(enumerate(subtitles), total=total_subtitles, 
                   desc=f"Translating subtitles ({source_language} to {target_language})", 
                   unit="subtitle") if not log_callback else enumerate(subtitles)
    
    for idx, subtitle in iterator:
        if check_interruption_callback and check_interruption_callback():
            if log_callback:
                log_callback("srt_translation_interrupted", 
                           f"Translation interrupted at subtitle {idx+1}/{total_subtitles}")
            else:
                tqdm.write(f"\nTranslation interrupted at subtitle {idx+1}/{total_subtitles}")
            break
        
        if progress_callback and total_subtitles > 0:
            progress_callback((idx / total_subtitles) * 100)
        
        text_to_translate = subtitle['text'].strip()
        
        if not text_to_translate:
            translations[idx] = ""
            completed_count += 1
            continue
        
        context_before = ""
        context_after = ""
        
        if idx > 0 and idx-1 in translations:
            context_before = translations[idx-1]
        elif idx > 0:
            context_before = subtitles[idx-1].get('text', '')
        
        if idx < len(subtitles) - 1:
            context_after = subtitles[idx+1].get('text', '')
        
        translated_text = await generate_translation_request(
            text_to_translate,
            context_before,
            context_after,
            "",
            source_language,
            target_language,
            model_name,
            llm_client=llm_client,
            log_callback=log_callback,
            custom_instructions=custom_instructions
        )
        
        if translated_text is not None:
            # Apply post-processing if enabled
            if enable_post_processing:
                if log_callback:
                    log_callback("post_processing_subtitle", f"Post-processing subtitle {idx+1}")
                
                # Get context for post-processing
                pp_context_before = ""
                pp_context_after = ""
                
                # Get previous translated subtitle as context
                if idx > 0 and idx-1 in translations:
                    pp_context_before = translations[idx-1]
                
                # Get next subtitle text as context (not yet translated)
                if idx < len(subtitles) - 1:
                    pp_context_after = subtitles[idx+1].get('text', '')
                
                improved_text = await post_process_translation(
                    translated_text,
                    target_language,
                    model_name,
                    llm_client=llm_client,
                    log_callback=log_callback,
                    custom_instructions=post_processing_instructions,
                    context_before=pp_context_before,
                    context_after=pp_context_after
                )
                translated_text = improved_text
            
            translations[idx] = translated_text
            completed_count += 1
        else:
            # Keep original text if translation fails
            err_msg = f"Failed to translate subtitle {idx+1}"
            if log_callback:
                log_callback("srt_subtitle_error", err_msg)
            else:
                tqdm.write(f"\n{err_msg}")
            translations[idx] = text_to_translate  # Keep original
            failed_count += 1
        
        if stats_callback and total_subtitles > 0:
            stats_callback({
                'completed_subtitles': completed_count,
                'failed_subtitles': failed_count,
                'total_subtitles': total_subtitles
            })
    
    if log_callback:
        log_callback("srt_translation_complete", 
                    f"Completed translation: {completed_count} successful, {failed_count} failed")
    
    return translations


async def translate_subtitles_in_blocks(subtitle_blocks: List[List[Dict[str, str]]], 
                                      source_language: str, target_language: str, 
                                      model_name: str, api_endpoint: str,
                                      progress_callback=None, log_callback=None, 
                                      stats_callback=None, check_interruption_callback=None,
                                      custom_instructions="", llm_provider="ollama", 
                                      gemini_api_key=None, enable_post_processing=False,
                                      post_processing_instructions="") -> Dict[int, str]:
    """
    Translate subtitle entries in blocks for better context preservation.
    
    Args:
        subtitle_blocks: List of subtitle blocks (each block is a list of subtitle dicts)
        source_language: Source language
        target_language: Target language
        model_name: LLM model name
        api_endpoint: API endpoint
        progress_callback: Progress update callback
        log_callback: Logging callback
        stats_callback: Statistics update callback
        check_interruption_callback: Interruption check callback
        custom_instructions: Additional translation instructions
        
    Returns:
        dict: Mapping of subtitle index to translated text
    """
    from src.core.srt_processor import SRTProcessor
    srt_processor = SRTProcessor()
    
    total_blocks = len(subtitle_blocks)
    total_subtitles = sum(len(block) for block in subtitle_blocks)
    translations = {}
    completed_count = 0
    failed_count = 0
    previous_translation_block = ""
    
    if log_callback:
        log_callback("srt_block_translation_start", 
                    f"Starting block translation: {total_subtitles} subtitles in {total_blocks} blocks...")
    
    # Create LLM client based on provider or custom endpoint
    llm_client = _create_llm_client(llm_provider, gemini_api_key, api_endpoint, model_name)
    
    for block_idx, block in enumerate(subtitle_blocks):
        if check_interruption_callback and check_interruption_callback():
            if log_callback:
                log_callback("srt_translation_interrupted", 
                           f"Translation interrupted at block {block_idx+1}/{total_blocks}")
            else:
                tqdm.write(f"\nTranslation interrupted at block {block_idx+1}/{total_blocks}")
            break
        
        if progress_callback and total_blocks > 0:
            progress_callback((block_idx / total_blocks) * 100)
        
        # Prepare subtitle blocks with indices
        subtitle_tuples = []
        block_indices = []
        
        for subtitle in block:
            idx = int(subtitle['number']) - 1  # Convert to 0-based index
            text = subtitle['text'].strip()
            if text:  # Only include non-empty subtitles
                subtitle_tuples.append((idx, text))
                block_indices.append(idx)
        
        if not subtitle_tuples:
            continue
        
        # Generate prompt for this block
        prompt = generate_subtitle_block_prompt(
            subtitle_tuples,
            previous_translation_block,
            source_language,
            target_language,
            TRANSLATE_TAG_IN,
            TRANSLATE_TAG_OUT,
            custom_instructions
        )
        
        # Make translation request using LLM client
        try:
            print("\n-------SENT to LLM-------")
            print(prompt)
            print("-------SENT to LLM-------\n")
            
            # Use provided client or default
            client = llm_client or default_client
            full_raw_response = await client.make_request(prompt, model_name)
            
            print("\n-------LLM RESPONSE-------")
            print(full_raw_response or "None")
            print("-------LLM RESPONSE-------\n")
            
            if full_raw_response:
                translated_block_text = client.extract_translation(full_raw_response)
            else:
                translated_block_text = None
                    
        except Exception as e:
            if log_callback:
                log_callback("srt_block_translation_error", f"Error: {str(e)}")
            translated_block_text = None
        
        if translated_block_text:
            # Extract individual translations from block
            block_translations = srt_processor.extract_block_translations(
                translated_block_text, block_indices
            )
            
            # Apply post-processing if enabled
            if enable_post_processing:
                for idx, trans_text in block_translations.items():
                    if log_callback:
                        log_callback("post_processing_subtitle", f"Post-processing subtitle {idx+1}")
                    
                    # Get context for post-processing
                    pp_context_before = ""
                    pp_context_after = ""
                    
                    # Get previous translated subtitle as context
                    if idx > 0:
                        # Check if previous subtitle is in current block translations
                        if idx-1 in block_translations:
                            pp_context_before = block_translations[idx-1]
                        # Otherwise check if it was translated in a previous block
                        elif idx-1 in translations:
                            pp_context_before = translations[idx-1]
                    
                    # Get next subtitle as context (within current block)
                    if idx+1 in block_translations:
                        pp_context_after = block_translations[idx+1]
                    else:
                        # Look for next subtitle in the original subtitles
                        for subtitle in subtitle_blocks[block_idx:]:
                            for sub in subtitle:
                                if int(sub['number']) - 1 == idx + 1:
                                    pp_context_after = sub['text']
                                    break
                            if pp_context_after:
                                break
                    
                    improved_text = await post_process_translation(
                        trans_text,
                        target_language,
                        model_name,
                        llm_client=llm_client,
                        log_callback=log_callback,
                        custom_instructions=post_processing_instructions,
                        context_before=pp_context_before,
                        context_after=pp_context_after
                    )
                    block_translations[idx] = improved_text
            
            # Update translations dictionary
            for idx, trans_text in block_translations.items():
                translations[idx] = trans_text
                completed_count += 1
            
            # Track failed translations in block
            for idx in block_indices:
                if idx not in block_translations:
                    # Keep original text for missing translations
                    for subtitle in block:
                        if int(subtitle['number']) - 1 == idx:
                            translations[idx] = subtitle['text']
                            failed_count += 1
                            break
            
            # Store translated block for context (last 5 subtitles)
            last_subtitles = []
            for idx in sorted(block_translations.keys())[-5:]:
                last_subtitles.append(f"[{idx}]{block_translations[idx]}")
            previous_translation_block = '\n'.join(last_subtitles)
            
        else:
            # Block translation failed - keep original text
            err_msg = f"Failed to translate block {block_idx+1}"
            if log_callback:
                log_callback("srt_block_error", err_msg)
            else:
                tqdm.write(f"\n{err_msg}")
            
            for subtitle in block:
                idx = int(subtitle['number']) - 1
                translations[idx] = subtitle['text']
                failed_count += 1
            
            previous_translation_block = ""  # Reset context on failure
        
        if stats_callback and total_subtitles > 0:
            stats_callback({
                'completed_subtitles': completed_count,
                'failed_subtitles': failed_count,
                'total_subtitles': total_subtitles,
                'completed_blocks': block_idx + 1,
                'total_blocks': total_blocks
            })
    
    if log_callback:
        log_callback("srt_block_translation_complete", 
                    f"Completed block translation: {completed_count} successful, {failed_count} failed")
    
    return translations